[
{
	"uri": "/010_intro.html",
	"title": "Learn KOP &amp; EKS - CloudWatch Overview",
	"tags": [],
	"description": "Official Rafay product documentation. Explore &#34;Learn KOP &amp; EKS - CloudWatch Overview&#34; docs and more here. Rafay is a SaaS-first Kubernetes Operations Platform with enterprise-class scalability.",
	"content": " In this self-paced exercise, you will learn how to standardize the configuration, deployment, and lifecycle management of CloudWatch Container Insights across a fleet of clusters using add-ons and blueprints.\nWhat Will You Do by Part    Part What will you do?     1 Setup and Configuration   2 Cluster Blueprint with Amazon CloudWatch Agent   3 Provision an Amazon EKS Cluster with Cloudwatch Agent   4 Deprovision the EKS cluster    Assumptions  You have access to an Amazon AWS account with privileges to create an IAM Role with the default Full IAM Policy to allow provisioning of resources on your behalf as part of the EKS cluster lifecycle.\n  "
},
{
	"uri": "/",
	"title": "Rafay Multi-Cluster Management Workshop",
	"tags": [],
	"description": "",
	"content": " AWS Modernization Workshop Welcome In this workshop you will learn how to automate your operations of EKS through Rafay\u0026rsquo;s Kubernetes Operations Platform.\nLearning Objectives  Provision a new EKS cluster with Rafay Kubernetes Operations Platform and import existing clusters for centralized operations. How to create centralized configurations for the look and feel of EKS clusters being managed across an enterprise. How to automate the upgrade of a new Kubernetes version across EKS clusters Enabling centralized zero trust access into Kubernetes clusters Audit access into EKS clusters and changes being made to EKS cluster  "
},
{
	"uri": "/10_step_e.html",
	"title": "Foreword",
	"tags": [],
	"description": "",
	"content": " How did we get here? Set the stage for the audience, this page isn\u0026rsquo;t mandatory but will can be a great page for a hook.\n"
},
{
	"uri": "/010_intro/1_setup.html",
	"title": "Learn KOP - Initial Setup",
	"tags": [],
	"description": "Official Rafay product documentation. Explore &#34;Learn KOP - Setup&#34; docs and more here. Rafay is a SaaS-first Kubernetes Operations Platform with enterprise-class scalability.",
	"content": " What Will You Do This is Part 1 of a multi-part, self-paced quick start exercise. In this part, you will perform a few \u0026ldquo;one-time\u0026rdquo; tasks required for cluster provisioning.\nStep 1: Create Cloud Credentials Cloud credentials provide privileges to programmatically interact with your Amazon AWS account so that the lifecycle of infrastructure associated with the Amazon EKS cluster can be managed.\n Follow the step-by-step instructions to create an IAM Role based cloud credential. Provide the name \u0026ldquo;aws-cloud-credential\u0026rdquo; for the cloud credential. Note, if a different name is used, the specification files will need to be updated to match the new name. \u0026mdash;  Step 2: Download RCTL The RCTL CLI allows you to programmatically interact with the controller enabling users to construct sophisticated automation workflows.\n Login into your Org Navigate to \u0026ldquo;My Tools\u0026rdquo; to download both the RCTL CLI and the \u0026ldquo;CLI Config\u0026rdquo; file Initialize RCTL using the step-by-step instructions Ensure you update your OS\u0026rsquo;s Path environment variable for RCTL  Step 3: Clone Git Repo Declarative specs for the Amazon EKS cluster and other resources are available in a Git repository\n Clone the Git repository to your laptop using the command below.\ngit clone https://github.com/RafaySystems/getstarted.git Once complete, you should see a folder called \u0026ldquo;cloudwatch\u0026rdquo; which contains the specs needed for this guide.\n  Recap At this point, you have everything setup and configured to provision the Amazon CloudWatch Agent enabled Amazon EKS Cluster\n"
},
{
	"uri": "/010_intro/2_provision.html",
	"title": "Learn KOP - Provision EKS Cluster with Amazon CloudWatch Integration",
	"tags": [],
	"description": "Official Rafay product documentation. Explore &#34;Learn KOP - Provision EKS Cluster with Amazon CloudWatch Integration&#34; docs and more here. Rafay is a SaaS-first Kubernetes Operations Platform with enterprise-class scalability.",
	"content": " What Will You Do In this part of the self-paced exercise, you will provision an Amazon EKS cluster based on a declarative cluster specification\nStep 1: Cluster Spec  Open Terminal (on macOS/Linux) or Command Prompt (Windows) and navigate to the folder where you forked the Git repository Navigate to the folder \u0026ldquo;/getstarted/cloudwatch/cluster\u0026rdquo;  The \u0026ldquo;cloudwatch-eks-cluster.yaml\u0026rdquo; file contains the declarative specification for our Amazon EKS Cluster.\nCluster Details The following items may need to be updated/customized if you made changes to these or used alternate names.\n cluster name: \u0026ldquo;cloudwatch-cluster\u0026rdquo; project: \u0026ldquo;defaultproject\u0026rdquo; blueprint: \u0026ldquo;cloudwatch-blueprint\u0026rdquo; blueprintversion: \u0026ldquo;v1\u0026rdquo; cloud provider: \u0026ldquo;aws-cloud-credential\u0026rdquo; name: \u0026ldquo;cloudwatch-cluster\u0026rdquo; region: \u0026ldquo;us-west-1\u0026rdquo;\nkind: Cluster metadata: name: cloudwatch-cluster project: defaultproject spec: blueprint: cloudwatch-blueprint blueprintversion: v1 cloudprovider: aws-cloud-credential cniprovider: aws-cni type: eks --- apiVersion: rafay.io/v1alpha5 kind: ClusterConfig metadata: name: cloudwatch-cluster region: us-west-1 tags: version: \u0026#34;1.21\u0026#34; managedNodeGroups: - name: ng-1 instanceType: t3.large desiredCapacity: 2 iam: withAddonPolicies: albIngress: true autoScaler: true efs: true cloudWatch: true  Step 2: Provision Cluster  Type the command below to provision the EKS cluster\nrctl apply -f cloudwatch-eks-cluster.yaml  If there are no errors, you will be presented with a \u0026ldquo;Task ID\u0026rdquo; that you can use to check progress/status. Note that this step requires creation of infrastructure in your AWS account and can take ~20-30 minutes to complete.\n{ \u0026#34;taskset_id\u0026#34;: \u0026#34;empr728\u0026#34;, \u0026#34;operations\u0026#34;: [ { \u0026#34;operation\u0026#34;: \u0026#34;NodegroupCreation\u0026#34;, \u0026#34;resource_name\u0026#34;: \u0026#34;ng-1\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;PROVISION_TASK_STATUS_PENDING\u0026#34; }, { \u0026#34;operation\u0026#34;: \u0026#34;ClusterCreation\u0026#34;, \u0026#34;resource_name\u0026#34;: \u0026#34;cloudwatch-cluster\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;PROVISION_TASK_STATUS_PENDING\u0026#34; } ], \u0026#34;comments\u0026#34;: \u0026#34;The status of the operations can be fetched using taskset_id\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;PROVISION_TASKSET_STATUS_PENDING\u0026#34; }  Navigate to the \u0026ldquo;defaultproject\u0026rdquo; project in your Org Click on Infrastructure -\u0026gt; Clusters. You should see something like the following   Click on the cluster name to monitor progress  Step 3: Verify Cluster Once provisioning is complete, you should see a healthy cluster in the web console\n Click on the kubectl link and type the following command\nkubectl get nodes  You should see something like the following\nNAME STATUS ROLES AGE VERSION ip-192-168-36-4.us-west-1.compute.internal Ready \u0026lt;none\u0026gt; 22m v1.21.5-eks-bc4871b ip-192-168-80-129.us-west-1.compute.internal Ready \u0026lt;none\u0026gt; 22m v1.21.5-eks-bc4871b Step 4: Verify CloudWatch Agent Now, let us verify the CloudWatch agent resources are operational on the EKS cluster\n Click on the kubectl link and type the following command\nkubectl get pod -n amazon-cloudwatch  You should see something like the following\nNAME READY STATUS RESTARTS AGE cloudwatch-agent-gxdr8 1/1 Running 0 10m cloudwatch-agent-zc8pw 1/1 Running 0 10m Step 5: Verify Metrics Verify that the agents are connected to CloudWatch and reporting data.\n In the AWS console, go to CloudWatch -\u0026gt; Insights -\u0026gt; Container Insights and search for the cluster \u0026ldquo;Cloudwatch-cluster\u0026rdquo; View the metrics for the cluster in Amazon CloudWatch  Recap Congratulations! At this point, you have successfully configured and provisioned an Amazon EKS cluster with the Amazon CloudWatch agent in your AWS account using the RCTL CLI.\n"
},
{
	"uri": "/010_intro/3_ztka.html",
	"title": "Learn KOP - Part 3 - Zero Trust Kubectl",
	"tags": [],
	"description": "Official Rafay product documentation. Explore &#34;Learn KOP - Part 3 - Zero Trust Kubectl&#34; docs and more here. Rafay is a SaaS-first Kubernetes Operations Platform with enterprise-class scalability.",
	"content": " This is Part 3 of a multi-part, self paced exercise.\nWhat Will You Do In part 3, you will\n Login into the Org simulating a remote user (e.g. a developer/colleague who is perhaps 100s or 1000s of miles away on a completely different network) Remotely access this cluster using the integrated browser based Zero Trust Kubectl View Kubectl audit logs  Estimated Time\nEstimated time burden for this part is 10 minutes.\nStep 1: Access Org  Login into the Org with your Org Admin credentials Navigate to the \u0026ldquo;desktop\u0026rdquo; project and Infrastructure -\u0026gt; Clusters  Step 2: Zero Trust Kubectl  Click on the \u0026ldquo;Kubectl\u0026rdquo; link on the cluster. This will provide you with a web based, zero trust kubectl shell. Type in a kubectl command such as \u0026ldquo;kubectl get ns\u0026rdquo; to get a response from the remote cluster on your desktop operating behind a NAT/firewall.  The controller injects a service account (sa) \u0026ldquo;Just In Time(JIT)\u0026rdquo; on the target cluster. The service account is automatically configured with the user\u0026rsquo;s role in the Org. You can view the JIT service account by using the following command. In the example below, the service account for the user \u0026ldquo;demos@rafay.co\u0026rdquo; was created just \u0026ldquo;5 seconds\u0026rdquo; back as the user opened the web based kubectl console. The service account is automatically removed from the target cluster once the configured lifetime expires.\nkubectl get sa -n rafay-system NAME SECRETS AGE default 1 53m demos-64rafay-46co 1 5s system-sa 1 53m Step 3: Audit Logs All commands performed using the zero trust kubectl channel are centralized through the controller. As a result, a complete audit trail of \u0026ldquo;who did what and when\u0026rdquo; is maintained. Administrators can view these audit logs.\n Click on Home -\u0026gt; System -\u0026gt; Audit Logs Click on the Kubectl tab  Audit logs can be viewed by \u0026ldquo;API\u0026rdquo; or \u0026ldquo;Commands\u0026rdquo; (for web based shell). See an example of the audit logs below.\nRecap Congratulations! In this part, you\n Experienced how remote users can securely access kubernetes clusters behind firewalls using zero trust kubectl Viewed the centralized audit logs for all kubectl based commands performed by users on managed clusters.  "
},
{
	"uri": "/010_intro/4_namespaces.html",
	"title": "Learn KOP - Part 4 - Namespaces",
	"tags": [],
	"description": "Official Rafay product documentation. Explore &#34;Learn KOP - Part 5 - Namespace Management&#34; docs and more here. Rafay is a SaaS-first Kubernetes Operations Platform with enterprise-class scalability.",
	"content": " This is Part 4 of a multi-part, self paced exercise.\nWhat Will You Do In this part, you will\n Configure a Kubernetes namespace spec in your project Publish this namespace on a fleet of Kubernetes clusters  Estimated Time\nEstimated time burden for this part is 10 minutes.\nStep 1: Create Namespace  Login into your Org and navigate to the \u0026ldquo;desktop\u0026rdquo; project Select Infrastructure -\u0026gt; Namespaces Click on \u0026ldquo;New Namespace\u0026rdquo; Enter \u0026ldquo;kubeless\u0026rdquo; for Name Select \u0026ldquo;Wizard\u0026rdquo; for Type and Save  !!! Note In addition to the Namespace wizard, users can also provide the k8s YAML spec for the namespace either by uploading it or point the controller to a Git repo where it can retrieve it.\nStep 2: Configure Namespace You will be presented with an intuitive wizard that you can use to configure your namespace\u0026rsquo;s requirements. In our case, we want to add labels to our namespace.\n Click on Labels -\u0026gt; Key-Value Provide k8s compliant text for the key and value Save  In the example below, we have entered \u0026ldquo;key=addon\u0026rdquo; and \u0026ldquo;value=kubeless\u0026rdquo;\nStep 3: Select Placement Since we only have one cluster in our project for this exercise, we cannot perform multi cluster operations.\n Select \u0026ldquo;Specific Clusters\u0026rdquo; for Placement Policy Select your cluster Click \u0026ldquo;SAVE \u0026amp; GO TO PUBLISH\u0026rdquo;  Step 4: Publish Namespace Click on Publish. In a few seconds, the configured namespace will be deployed on the target clusters. Note that the target clusters can be in completely separate security domains and the controller can still manage namespace lifecyle remotely.\nStep 5: Verify Namespace Optionally, you can verify what the published namespace looks like on your cluster.\n Navigate to Infrastructure -\u0026gt; Clusters Click on Kubectl  In the example below, you can see that the \u0026ldquo;kubeless\u0026rdquo; namespace was created on the cluster a few seconds back when we published it.\nkubectl get ns NAME STATUS AGE default Active 40h kube-node-lease Active 40h kube-public Active 40h kube-system Active 40h kubeless Active 34s rafay-infra Active 13h rafay-system Active 16h You can also look deeper into the namespace by describing it. Notice that the the \u0026ldquo;custom label\u0026rdquo; we specified is part of the namespace.\nkubectl describe ns kubeless Name: kubeless Labels: addon=kubeless app.kubernetes.io/managed-by=Helm kubernetes.io/metadata.name=kubeless name=kubeless rafay.dev/auxiliary=true rafay.dev/component=namespace rafay.dev/global=true rafay.dev/name=namespace rep-cluster=k5zpglk rep-cluster-name=desktop rep-drift-reconcillation=disabled rep-organization=lk5od2e rep-partner=rx28oml rep-placement=mx6ox7m rep-project=kgxy58m rep-project-name=desktop rep-system-managed=true rep-workload=namespace-kj351lm-kubeless rep-workloadid=qkoz7nm Annotations: meta.helm.sh/release-name: namespace-kj351lm-kubeless meta.helm.sh/release-namespace: kubeless rafay.dev/resource-hash: efbeb365597980119ab784ad5eba8d1baf2d8c7cb8b78605d1020974c118de5f rep-drift-action: deny Status: Active No resource quota. No LimitRange resource. Recap Congratulations! At this point, you have successfully configured and published a namespace to your Kubernetes cluster. You also verified the namespace\u0026rsquo;s specification directly on the cluster using Kubectl.\n"
},
{
	"uri": "/010_intro/5_blueprint.html",
	"title": "Learn KOP - Amazon CloudWatch Blueprint",
	"tags": [],
	"description": "Official Rafay product documentation. Explore &#34;Learn KOP - Amazon CloudWatch Blueprint&#34; docs and more here. Rafay is a SaaS-first Kubernetes Operations Platform with enterprise-class scalability.",
	"content": " What Will You Do In this part of the self-paced exercise, you will create a custom cluster blueprint with a Amazon CloudWatch Agent add-on, based on declarative specifications.\nStep 1: Create Repository In this step, you will create a repository in your project so that the controller can retrieve the Helm charts automatically.\n Open Terminal (on macOS/Linux) or Command Prompt (Windows) and navigate to the folder where you forked the Git repository Navigate to the folder \u0026ldquo;/getstarted/cloudwatch/repository\u0026rdquo;  The \u0026ldquo;cloudwatch-repository.yaml\u0026rdquo; file contains the declarative specification for the repository. In this case, the specification is of type \u0026ldquo;Helm Repository\u0026rdquo; and the \u0026ldquo;endpoint\u0026rdquo; is pointing to the AWS Github repository that includes the CloudWatch Helm chart.\napiVersion: config.rafay.dev/v2 kind: Repository metadata: name: cloudwatch-repo spec: repositoryType: HelmRepository endpoint: https://aws.github.io/eks-charts credentialType: CredentialTypeNotSet Type the command below\nrctl create repository -f cloudwatch-repository.yaml If you did not encounter any errors, you can optionally verify if everything was created correctly on the controller.\n Navigate to the \u0026ldquo;defaultproject\u0026rdquo; project in your Org Select Integrations -\u0026gt; Repositories  Step 2: Create Namespace In this step, you will create a namespace for the Cloudwatch agent. The \u0026ldquo;cloudwatch-namespace.yaml\u0026rdquo; file contains the declarative specification\nThe following items may need to be updated/customized if you made changes to these or used alternate names.\n value: cloudwatch-cluster\nkind: ManagedNamespace apiVersion: config.rafay.dev/v2 metadata: name: amazon-cloudwatch description: namespace for Amazon Cloudwatch labels: annotations: spec: type: RafayWizard resourceQuota: placement: placementType: ClusterSpecific clusterLabels: - key: rafay.dev/clusterName value: cloudwatch-cluster Open Terminal (on macOS/Linux) or Command Prompt (Windows) and navigate to the folder where you forked the Git repository\n Navigate to the folder \u0026ldquo;/getstarted/cloudwatch/namespace\u0026rdquo;\n Type the command below\nrctl create namespace -f cloudwatch-namespace.yaml  If you did not encounter any errors, you can optionally verify if everything was created correctly on the controller.\n Navigate to the \u0026ldquo;defaultproject\u0026rdquo; project in your Org Select Infrastructure -\u0026gt; Namespaces You should see an namesapce called \u0026ldquo;amazon-cloudwatch\u0026rdquo;  Step 3: Create Addon In this step, you will create a custom addon for the Cloudwatch Agent. The \u0026ldquo;cloudwatch-addon.yaml\u0026rdquo; file contains the declarative specification\nIf you plan to use a different name for the cluster other than \u0026ldquo;cloudwatch-cluster\u0026rdquo;, you must update the \u0026ldquo;custom-values.yaml\u0026rdquo; file located in the folder \u0026ldquo;/getstarted/cloudwatch/addon\u0026rdquo; with the name of the cluster\nThe following details are used to build the declarative specification.\n \u0026ldquo;v1\u0026rdquo; because this is our first version The addon is part of the \u0026ldquo;defaultproject\u0026rdquo; Name of addon is \u0026ldquo;cloudwatch-addon\u0026rdquo; The addon will be deployed to a namespace called \u0026ldquo;amazon-cloudwatch\u0026rdquo; You will be using a custom \u0026ldquo;custom-values.yaml as an override which is located in the folder \u0026ldquo;/getstarted/cloudwatch/addon\u0026rdquo;\n The \u0026ldquo;aws-cloudwatch-metrics\u0026rdquo; chart will be used from the previously created repository named \u0026ldquo;cloudwatch-repo\u0026rdquo;  The following items may need to be updated/customized if you made changes to these or used alternate names.\n repository_ref: \u0026ldquo;cloudwatch-repo\u0026rdquo;\nkind: AddonVersion metadata: name: v1 project: defaultproject spec: addon: cloudwatch-addon namespace: amazon-cloudwatch template: type: Helm3 valuesFile: custom-values.yaml repository_ref: cloudwatch-repo repo_artifact_meta: helm: chartName: aws-cloudwatch-metrics Open Terminal (on macOS/Linux) or Command Prompt (Windows) and navigate to the folder where you forked the Git repository\n Navigate to the folder \u0026ldquo;/getstarted/cloudwatch/addon\u0026rdquo;\n Type the command below\nrctl create addon version -f cloudwatch-addon.yaml  If you did not encounter any errors, you can optionally verify if everything was created correctly on the controller.\n Navigate to the \u0026ldquo;defaultproject\u0026rdquo; project in your Org Select Infrastructure -\u0026gt; Addons You should see an addon called \u0026ldquo;cloudwatch-addon\u0026rdquo;  Step 4: Create Blueprint In this step, you will create a custom cluster blueprint with the CloudWatch addon. The \u0026ldquo;cloudwatch-blueprint.yaml\u0026rdquo; file contains the declarative specification.\n Open Terminal (on macOS/Linux) or Command Prompt (Windows) and navigate to the folder where you forked the Git repository Navigate to the folder \u0026ldquo;/getstarted/cloudwatch/blueprint\u0026rdquo;\n  The following items may need to be updated/customized if you made changes to these or used alternate names.\n project: \u0026ldquo;defaultproject\u0026rdquo;\nkind: Blueprint metadata: # blueprint name name: cloudwatch-blueprint #project name project: defaultproject Type the command below\nrctl create blueprint -f cloudwatch-blueprint.yaml  If you did not encounter any errors, you can optionally verify if everything was created correctly on the controller.\n Navigate to the \u0026ldquo;defaultproject\u0026rdquo; project in your Org Select Infrastructure -\u0026gt; Blueprint You should see an blueprint called \u0026ldquo;cloudwatch-blueprint  New Version Although we have a custom blueprint, we have not provided any details on what it comprises. In this step, you will create and add a new version to the custom blueprint. The YAML below is a declarative spec for the new version.\nThe following items may need to be updated/customized if you made changes to these or used alternate names.\n project: \u0026ldquo;defaultproject\u0026rdquo; blueprint: \u0026ldquo;cloudwatch-blueprint\u0026rdquo; name: \u0026ldquo;cloudwatch-addon\u0026rdquo; version: \u0026ldquo;v1\u0026rdquo;\nkind: BlueprintVersion metadata: name: v1 project: defaultproject description: Amazon CloudWatch Agent spec: blueprint: cloudwatch-blueprint baseSystemBlueprint: default baseSystemBlueprintVersion: \u0026#34;\u0026#34; addons: - name: cloudwatch-addon version: v1 # cluster-scoped or namespace-scoped pspScope: cluster-scoped rafayIngress: true rafayMonitoringAndAlerting: false kubevirt: false # BlockAndNotify or DetectAndNotify driftAction: BlockAndNotify Type the command below to add a new version\nrctl create blueprint version -f cloudwatch-blueprint-v1.yaml  If you did not encounter any errors, you can optionally verify if everything was created correctly on the controller.\n Navigate to the \u0026ldquo;defaultproject\u0026rdquo; project in your Org Select Infrastructure -\u0026gt; Blueprint Click on the \u0026ldquo;cloudwatch-blueprint\u0026rdquo; custom cluster blueprint  Recap As of this step, you have created a \u0026ldquo;cluster blueprint\u0026rdquo; with the CloudWatch agent as one of the addons. You are now ready to move onto the next step where you will provision an EKS cluster with this custom cluster blueprint.\nNote that you can also reuse this cluster blueprint for as many clusters as you require in this project and also share the blueprint with other projects.\n"
},
{
	"uri": "/010_intro/6_visandmon.html",
	"title": "Learn KOP - Part 6 - Visibility and Monitoring",
	"tags": [],
	"description": "Official Rafay product documentation. Explore &#34;Learn KOP - Part 6 - Visibility and Monitoring&#34; docs and more here. Rafay is a SaaS-first Kubernetes Operations Platform with enterprise-class scalability.",
	"content": " This is Part 6 of a multi-part, self-paced exercise.\nWhat Will You Do In this part, you will explore the integrated visibility and monitoring capabilities of the platform. Specifically, you will explore the dashboards that provide you access to both critical summary and trends.\n You will start with a \u0026ldquo;bird\u0026rsquo;s eye view\u0026rdquo; and contextually click in one level at a time, going deeper and deeper. Critical metrics are automatically scraped and aggregated at the controller in a centralized time series database (TSDB) Interactive, real time access to this data is provided  Estimated Time\nEstimated time burden for this part is 15 minutes.\n!!! Important This part requires the \u0026ldquo;monitoring addon\u0026rdquo; to be enabled in the cluster blueprint. k8s on Docker Desktop is incompatible with this addon.\nOrg Dashboard Click on Home -\u0026gt; Dashboard to view all clusters, projects, user activity, resource utilization and events in the Project. You should see something like the example below.\nProject Dashboard Select a Project and Click on Dashboard to view all clusters, projects, user activity, application, workloads and events across the entire Project. You should see something like the example below.\nCluster Dashboard In your project, click on Infrastructure -\u0026gt; Clusters. You should see something like the example below providing an overview of critical, operational metrics for your cluster.\nNode Dashboard In the cluster dashboard, click on Nodes. This will provide you an overview of all the nodes in the cluster. You should see something like the example below.\nClick on \u0026ldquo;Overview\u0026rdquo; for one of your nodes. This will provide you with a dashboard for the node.\nk8s Resources Click on the \u0026ldquo;Resources\u0026rdquo; tab. This will provide you access to an \u0026ldquo;integrated Kubernetes dashboard\u0026rdquo; where you can view the k8s resources organized by type, by namespace etc.\nPod Dashboard By default, the k8s dashboard will list the pods in the \u0026ldquo;kube-system\u0026rdquo; namespace. Click on the \u0026ldquo;coredns\u0026rdquo; pod to view the Pod dashboard. You should see something like the example below.\nContainer Dashboard Click on \u0026ldquo;Configuration\u0026rdquo; for the pod in the pod dashboard\nNow, click on the container in the configuration to go to the \u0026ldquo;container dashboard\u0026rdquo;. You should see something like the example below.\nRecap Congratulations! At this point, you have successfully accessed the integrated dashboards for visibility and monitoring.\n"
},
{
	"uri": "/010_intro/7_polmgmt.html",
	"title": "Learn KOP - Part 8 - Policy Management",
	"tags": [],
	"description": "Official Rafay product documentation. Explore &#34;Learn KOP - Part 8 - Policy Management&#34; docs and more here. Rafay is a SaaS-first Kubernetes Operations Platform with enterprise-class scalability.",
	"content": " This is Part 8 of a multi-part, self-paced exercise.\nWhat Will You Do In this part, you will create a Kubernetes policy in your project and enforce it on all cluster in your project. You will deploy a workload out of compliance with the policy and notice that it is blocked from deployment to the cluster.\nEstimated Time\nEstimated time burden for this part is 15 minutes.\nStep 1: Setup Workload One of the key benefits of using OPA Gatekeeper for policy management is that in addition to enforcing policies during \u0026ldquo;admission\u0026rdquo; to the Kubernetes cluster, it can also be used to determine which k8s resources \u0026ldquo;already existing\u0026rdquo; on the cluster are \u0026ldquo;out of compliance\u0026rdquo; with policy.\nIn this step, we will ensure we deploy a workload from Part-7 with a specific number of replicas that will be out of compliance with our policy. We will use this to show how you can identify resources in violation of policy.\n Ensure that the \u0026ldquo;replicaCount\u0026rdquo; in the values.yaml file is set to \u0026ldquo;2\u0026rdquo;\n# Default values for webserver. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 2 nginx: name: frontend image:  On your cluster, when you kubectl in the workload\u0026rsquo;s namespace, you should see something like the following.\nkubectl get po -n first NAME READY STATUS RESTARTS AGE test-helm-webserver-77585694cc-29vtc 2/2 Running 11 10s test-helm-webserver-77585694cc-qkvjd 2/2 Running 11 10s Step 2: Constraint Template You can have as many policies as you want in a Project in an Org. An OPA policy comprises one or many OPA Gatekeeper constraints. In this step, we will create a custom constrain template which will require all \u0026ldquo;k8s deployments\u0026rdquo; deployed on the cluster to have a \u0026ldquo;Replica Count\u0026rdquo; within the \u0026ldquo;specified limit\u0026rdquo;.\nConstraint Template\n Save the YAML File on your laptop In your project, navigate to OPA Gatekeeper -\u0026gt; Constraint Templates Althought a number of templates are provided out of the box, we want to create a custom template Click on new template Enter \u0026ldquo;k8sreplicalimits\u0026rdquo; and select \u0026ldquo;upload file\u0026rdquo; for artifact sync Upload the YAML file you saved above  !!! Important Ensure that the name you provide for the template matches the name in the YAML file\napiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8sreplicalimits annotations: description: Requires a number of replicas to be set for a deployment between a min and max value. spec: crd: spec: names: kind: k8sreplicalimits validation: # Schema for the `parameters` field openAPIV3Schema: type: object properties: ranges: type: array items: type: object properties: min_replicas: type: integer max_replicas: type: integer targets: - target: admission.k8s.gatekeeper.sh rego: |package k8sreplicalimits deployment_name = input.review.object.metadata.name violation[{\u0026#34;msg\u0026#34;: msg}] { spec := input.review.object.spec not input_replica_limit(spec) msg := sprintf(\u0026#34;The provided number of replicas is not allowed for deployment: %v. Allowed ranges: %v\u0026#34;, [deployment_name, input.parameters]) } input_replica_limit(spec) { provided := input.review.object.spec.replicas count(input.parameters.ranges) \u0026gt; 0 range := input.parameters.ranges[_] value_within_range(range, provided) } value_within_range(range, value) { range.min_replicas \u0026lt;= value range.max_replicas \u0026gt;= value }  Step 3: Constraint In this step, we will create a \u0026ldquo;custom constraint\u0026rdquo; based on the \u0026ldquo;constraint template\u0026rdquo; from the previous step. This constraint allows you to \u0026ldquo;define/specify\u0026rdquo; a MIN and MAX replicas for all deployments on your cluster.\n Download the YAML File to your laptop Navigate to OPA Gatekeeper -\u0026gt; Constraints Click on new constraint Enter \u0026ldquo;replica-limits\u0026rdquo; and select \u0026ldquo;upload file\u0026rdquo; for artifact sync Upload the YAML file you saved above  !!! Important Ensure that the name you provide for the constraint matches the name in the YAML file\napiVersion: constraints.gatekeeper.sh/v1beta1 kind: k8sreplicalimits metadata: name: replica-limits spec: match: kinds: - apiGroups: [\u0026#34;apps\u0026#34;] kinds: [\u0026#34;Deployment\u0026#34;] parameters: ranges: - min_replicas: 3 max_replicas: 10 Step 4: Policy A policy comprises at least one constraints (default and/or custom) and optional exclusions.\n Navigate to OPA Gatekeeper -\u0026gt; Policy Click on new Policy, provide a name and Save Provide a version (alphanumeric is ok) Select the \u0026ldquo;replica-limits\u0026rdquo; constraint from the dropdown and Save  Step 5: Cluster Blueprint In the previous step, you created a policy comprising OPA Gatekeeper constraints. Now, you will include this policy in a \u0026ldquo;custom cluster blueprint\u0026rdquo; and apply it to your cluster. This step will be very similar to what you did in Part 5.\n!!! Important You can use the same cluster blueprint for multiple clusters achieving project and org wide standardization and compliance.\n Navigate to Infrastructure -\u0026gt; Blueprints Create a new blueprint and provide a name (e.g. opa) Click on new version Select \u0026ldquo;minimal\u0026rdquo; for base blueprint Select \u0026ldquo;Enable OPA Gatekeeper\u0026rdquo;, select the \u0026ldquo;OPA policy\u0026rdquo; and version we created in the previous step and Sav  Navigate to your cluster and apply the \u0026ldquo;OPA\u0026rdquo; custom cluster blueprint on the cluster. In a few minutes, all required components for OPA Gatekeeper and the constraints will become operational on your cluster.\nOptionally, you can also use kubectl to look at the OPA gatekeeper resources on the cluster\nkubectl get po -n rafay-system NAME READY STATUS RESTARTS AGE controller-manager-868759489b-f6xpg 1/1 Running 30 35h edge-client-75d456497-78tqp 1/1 Running 1 35h gatekeeper-audit-69d75d695c-rwnnd 1/1 Running 0 28h gatekeeper-controller-manager-59fb975495-rvnks 1/1 Running 0 25s gatekeeper-controller-manager-59fb975495-x2fpw 1/1 Running 0 25s gatekeeper-controller-manager-59fb975495-zj5dg 1/1 Running 0 25s rafay-connector-6c6c4f67cf-m5c6n 1/1 Running 44 35h relay-agent-7bb69f58c4-5chgc 1/1 Running 1 35h velero-dk6wpm1-desktop-55f57589ff-x6nf8 1/1 Running 0 23h To check if the OPA gatekeeper \u0026ldquo;constraints\u0026rdquo; we specified in the policy are operational on the cluster. As you can see from the results, OPA Gatekeeper was automatically deployed to the cluster with configuration from the policy we specified.\nkubectl get constraints NAME AGE replica-limits 5m Step 6: Policy Violations  Navigate to Home -\u0026gt; System -\u0026gt; Audit Logs Select the OPA tab Select the project and your cluster name (e.g. desktop project, desktop cluster)  This will perform a real time retrieval of \u0026ldquo;policy violations\u0026rdquo; already existing on the selected cluster and display the results to the administrator. In our example, we can see that it is reporting that our pre-existing workload has deployments that violate our policy. A logical approach will be for security administrators to work with the workload owners to remediate the violations.\nStep 7: New Deployments In this step, we will see what workload owners will experience if they try to deploy a new workload or update an existing workload that is not in compliance with the configured policy in the cluster blueprint.\n Navigate to your Git repo and update the \u0026ldquo;replicaCount\u0026rdquo; in the values.yaml file to \u0026ldquo;1\u0026rdquo; The GitOps pipeline will pick up the changes and attempt to deploy to the remote cluster The pipeline job will fail because the specs are \u0026ldquo;not in compliance\u0026rdquo; with the configured policy. Workload owners will be presented with a friendly error message as in the example shown below.  Recap Congratulations! At this point, you have successfully created a policy and tested enforcement on your Kubernetes cluster.\n"
},
{
	"uri": "/010_intro/8_deprov.html",
	"title": "Learn KOP - Deprovision Cluster",
	"tags": [],
	"description": "Official Rafay product documentation. Explore &#34;Learn KOP - Deprovision Cluster&#34; docs and more here. Rafay is a SaaS-first Kubernetes Operations Platform with enterprise-class scalability.",
	"content": " What Will You Do In this part of the self-paced exercise, you will deprovision your EKS cluster and all infrastructure that was deployed to your AWS account.\nDeprovision You can deprovision/delete your EKS cluster using the RCTL CLI.\nrctl delete cluster cloudwatch-cluster  Alternatively, you can also perform this from the web console\n Click on the gear icon on the far right of the cluster Select \u0026ldquo;Delete\u0026rdquo; and acknowledge  NOTE that the delete operation can take some time so that it can delete all the infrastructure associated with your EKS Cluster.\n"
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/010_intro/overview.html",
	"title": "Learn KOP - Overview",
	"tags": [],
	"description": "Official Rafay product documentation. Explore &#34;Learn KOP - Overview&#34; docs and more here. Rafay is a SaaS-first Kubernetes Operations Platform with enterprise-class scalability.",
	"content": " This is a multi-part, self paced quick start exercise that users can follow to experience the key capabilities of the platform on an imported Kubernetes cluster. This exercise has been specifically designed to be experienced with just a laptop and not require access to expensive infrastructure on public cloud environments.\nWhat Will You Do    Part What will you do?     1 Create a new Project in your Org and import a k8s cluster into the Project   2 Add a new user (ideally a remote colleague) to the new project. Configure this user with a specific role   3 Have the user perform kubectl operations on your k8s cluster using zero trust kubectl and view kubectl access audit logs   4 Configure and publish a namespace to remote Kubernetes clusters   5 Configure and publish a custom cluster blueprint and apply to your cluster   6 Integrated Dashboards for Visibility and Monitoring   7 Create and operate a GitOps pipeline for deployment automation   8 Create and enforce a k8s policy on your cluster   9 Create and implement a backup policy for your cluster    Assumptions This exercise assumes that you have access to the following:\n A laptop/desktop A reliable Internet connection (to download container images to your cluster etc.) No firewall/router blocking outgoing connections on port 443 A current version of kubectl CLI installed on your laptop An Org with Org Admin privileges An account on GitHub (to test/experience GitOps pipelines) An AWS Account (AWS S3 service) (to test/experience cluster backups)  "
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]